---
title: "DM Final Report_Group 19"
author: "Yang Le Lim (yanglel) & Mina Narayanan (mjnaraya) & Samvrudhi Shankar (samvrudv)"
date: "5/12/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
---

```{r setup, include=FALSE}
# load library 
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(randomForest)
library(tidyverse)
library(ggplot2)
library(ISLR)
library(MASS)
library(klaR) 
library(knitr)
library(glmnet)
library(gam)
library(plyr)
library(reshape)
library(boot)
library(survival)
library(ggfortify)
library(caret)
library(pROC)
library(ROSE)
library(tree)
library(partykit)
library(gridExtra)
options(scipen = 4)  # Suppresses scientific notation
```

> Disclaimer: We referenced ProPublica's methodology to determine whether the performance of our random forest model is equally predictive across race, age, and gender for the general recidivism and violent recidivism data (step 3 of our analysis). Specifically, we interpreted hazard ratios produced from the Cox Proportional Hazards Model to do this. Similarly, we referenced ProPublica's implementation of a Cox Proportional Hazards Model to compare the performance of our models to the COMPAS model (step 4 of our analysis).

Source: https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb

# A. Summary 

For this data mining project, we are looking at criminal recidivism prediction of done by COMPAS and Propublica.

Our analysis seeks to

1. Construct an RAI for predicting two-year recidivism on the Broward County population

    - Select the most salient predictors from **compas-scores-two-years.csv** to estimate the outcome variable **is_recid**

    - Split the data into test (40% of the data) and training (60% of the data) datasets and record training error

    - Revise the predictors in the model as needed

2. Construct an RAI for predicting two-year violent recidivism on the Broward County population

    - Select the most salient predictors from **compas-scores-two-years.csv** to estimate the outcome variable **is_violent_recid**

    - Split the data into test (40% of the data) and training (60% of the data) datasets and record training error

    - Revise the predictors in the model as needed

3. Determine whether each RAI is equally predictive across race, age, and gender

4. Compare the performance of our RAIs to COMPAS

     - Cox Proportional Hazards Model
     
     - Confusion Matrix 
     

> Task 1 & 2

For both general and violent recidivism, we have used the following predictors: gender, age, race, type of crime (misdemeanor or felony), number of prior crimes, number of juvenile crimes committed (misdemeanor, felony, other). 

For both general and violent recidivism, we have tried out the following models: LDA, QDA, decision tree, random forest, logistic regression,and KNN.

The most important predictors for general recidvism are age, race, number of prior crimes, and number of juvenile crimes (other). 

The most important predictors for violent recidvism are age, race, number of prior crimes, and gender. As compared to the general recidivism, the number of juvenile crimes (other) disppears and is replaced by gender. 

After comparing between the different models, the random forest model performs the best. (justification is in Task 4 below)

> Task 3

Our RAIs are not equally predictive across age and gender, which is demonstrated by the clear differences in hazard ratios for both general and violent recidivism.

For general and violent recidivism, there is a slight difference in the predictive power of our model across race, though this difference is not significant. 
> Task 4

Random forest has the highest accuracy and sensitivity for general recidivism. 

For violent recidivismRandom forest and QDA had the highest accuracies, but among these two models random forest had the higher sensitivity. 

We chose to use sensitivity as our second performance metric because in the context of identifying people who are likely to recidivate, we care most about identifying true positives. True positives are people we predict to be high risk for recidivism who do in fact recidivate. In comparison to the COMPAS model, the random forest model had a higher accuracy for general and violent recidivism. However, the random forest model had a lower sensitivity than the COMPAS model for general and violent recidivism.

# B. Preparation

## B.1 Read in compas data
```{r}
# Read in compas data
raw_data <- read.csv("./compas-scores-two-years.csv")
nrow(raw_data)

compas_non_violent <- raw_data %>% 
  filter(days_b_screening_arrest <= 30) %>%
  filter(days_b_screening_arrest >= -30) %>%
  filter(is_recid != -1) %>%
  filter(c_charge_degree != "O") %>%
  filter(score_text != 'N/A')
nrow(compas_non_violent)

compas_non_violent <- mutate(compas_non_violent, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 3)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor.1 = factor(score_text != "Low", labels = c("LowScore","HighScore"))) %>%
      mutate(v_score_factor.1 = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))

# Read in cox proportional hazards data
cox_parsed <- read.csv("./cox-parsed.csv")
```

## B.2 Create train and test datasets
```{r}
set.seed(1)
index = sample( 1:nrow( compas_non_violent ), round( nrow( compas_non_violent )*0.6 ), replace = FALSE )
train <- compas_non_violent[ index, ] %>% # About 60% of the observations
  arrange(id) 
test <- compas_non_violent[ -index, ] %>% # About 40% of the observations
  arrange(id) 
```

## B.3 Modelling  

We have used LDA, QDA, decision tree, random forest, logistic regression,and KNN to find a better model.

The models were first run on the training set before gathering the various metrics on the test set.

There are two outcomes of interest:
  
  1) general recidivism, which includes both non-violent and violent
  
  2) violent recidivism. 
  
The predictors in the dataset which can be used for a pre-arrest assessment are gender, age, race, type of crime (misdemeanor or felony), number of prior crimes, number of juvenile crimes committed (misdemeanor, felony, other). 

All these predictors will be used in our modelling. 

## B.4 Checking for correlation between input variables (continuous)
```{r}
# Check for correlation between input variables
compas.var.names <- c("priors_count", "juv_fel_count", "juv_misd_count", "juv_other_count")
round(cor(compas_non_violent[,compas.var.names]), 3)

# Function to check for correlation 
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = pmax(1, cex.cor * r))
}

pairs(compas_non_violent[,compas.var.names], lower.panel = panel.cor)
```

> There is no obvious correlation between the continuous predictors, which is good.

## B.5 Checking for balance

When we did a preliminary run of our models on violent recidivism, it cannot perform very well as the proportion of the majority class is too large (no violent recidivism). They tend to predict every observation as the majority class, as this can maximize accuracy more easily. This is at the cost of sensitivity as the actual violent recidivism is not predicted correctly.

To compensate for the lack of balance, we decided to use Synthetic Data Generation to create more observations for the minority class using the existing predictors. This can improve the performance of the model while retaining the distribution of the underlying dataset. See the source below for more information.

Source: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/

```{r}
# is_violent_recid is unbalanced
table(train$is_recid) # train general recidivisim (num)
prop.table(table(train$is_recid)) # train general recidivisim (proportion)
table(test$is_recid) # test general recidivisim (num)
prop.table(table(test$is_recid)) # test general recidivisim (proportion)
table(train$is_violent_recid) # train violent recidivisim (num)
prop.table(table(train$is_violent_recid)) # train violent recidivisim (proportion)
table(test$is_violent_recid)  # test violent recidivisim (num)
prop.table(table(test$is_violent_recid)) # test violent recidivisim (proportion)
```

#### Example of a model on violent recidivism in the training set
```{r}
violent.lda <- lda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

# Training Accuracy 
violent.lda.train = predict(violent.lda, type="response")
confusionMatrix(as.factor(violent.lda.train$class),as.factor(train$is_violent_recid),positive = "1")

# Testing Accuracy
violent.lda.test = predict(violent.lda, newdata=test)
confusionMatrix(as.factor(violent.lda.test$class),as.factor(test$is_violent_recid),positive = "1")
```

> As seen in the example above, sensitivity is very low (close to zero), even though there is high accuracy. The proportion of actual violent recidivism being correctly classified is low.  

#### Synthetic Data Generation
```{r, fig.width=11}
# to ensure synthetic data for continuous variables remains as an integer
train.1 <- train %>%
      mutate(priors_count_factor = factor(priors_count)) %>%
      mutate(juv_fel_count_factor = factor(juv_fel_count)) %>%
      mutate(juv_misd_count_factor = factor(juv_misd_count)) %>%
      mutate(juv_other_count_factor = factor(juv_other_count)) 

train.rose <- ROSE(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count_factor + juv_fel_count_factor + juv_misd_count_factor + juv_other_count_factor, data = train.1, seed = 1)$data

# reconvert the continuous variables back to a number type
train.rose <- train.rose %>%
  mutate(priors_count = as.numeric(as.character(priors_count_factor))) %>%
      mutate(juv_fel_count = as.numeric(as.character(juv_fel_count_factor))) %>%
      mutate(juv_misd_count = as.numeric(as.character(juv_misd_count_factor))) %>%
      mutate(juv_other_count = as.numeric(as.character(juv_other_count_factor))) 

table(train.rose$is_violent_recid) # train violent recidivisim with balancing (num)
prop.table(table(train.rose$is_violent_recid)) # train violent recidivisim with balancing (proportion)

# plotting the distribution of prior crime count 
plot.1 <- train.1 %>%
  filter(is_violent_recid==0) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Actual train set for prior crime (no violent recid)")

plot.2  <- train.1 %>%
  filter(is_violent_recid==1) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Actual train set for prior crime (violent recid)")

plot.3 <- train.rose %>%
  filter(is_violent_recid==0) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Synthetic train set for prior crime (no violent recid)")

plot.4 <- train.rose %>%
  filter(is_violent_recid==1) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Synthetic train set for prior crime (violent recid)")

grid.arrange(plot.1,plot.3, ncol=2)
grid.arrange(plot.2,plot.4, ncol=2)

```

>  After balancing, we now have similar proportions of outcomes in training set for violent recidivism. From the histogram of prior crimes, we can see that the synthetic data has similar distributions with the actual data.


## B.6 Creating Benchmark 

In the study, the Cox Model uses concordance as an indicator of accuracy. To create a comparable benchmark with Propublica, the csv input of Propublica's code will be restricted to observations that appear in our test set.

We will also use a confusion matrix on the test set as another benchmark.

#### Confusion matrix of the test set (for predictions made by compas) 
```{r}
# Revaluing the levels of low score to zero and high score to 1
test$score_factor.1 <- revalue(test$score_factor.1, c("LowScore" = 0, "HighScore" = 1))
test$v_score_factor.1 <- revalue(test$v_score_factor.1, c("LowScore" = 0, "HighScore" = 1))

# General Recidivism
a <- confusionMatrix(as.factor(test$score_factor.1),as.factor(test$is_recid),positive = "1")
a

# Violent Recidivism
b <- confusionMatrix(as.factor(test$v_score_factor.1),as.factor(test$is_violent_recid),positive = "1")
b

```

#### Adapting Propublica's Cox Model code to create benchmark
```{r}
# obtain subset of csv input of Propublica's code
cox.test <- semi_join(cox_parsed,test,by="id")
cox.test <- filter(filter(cox.test, score_text != "N/A", v_score_text != "N/A"), end > start) %>%
        mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>% 
        within(race_factor <- relevel(race_factor, ref = 3)) %>% 
        mutate(score_factor = factor(score_text)) %>%
        within(score_factor <- relevel(score_factor, ref=2)) %>%
        mutate(v_score_factor = factor(v_score_text)) %>%
        within(v_score_factor <- relevel(v_score_factor, ref=2)) %>%
        mutate(event.gen = event) %>%
        mutate(event.vio = if_else(event.gen == 0, as.integer(0), is_violent_recid)) %>%
        mutate(age_factor = as.factor(age_cat)) %>%
        within(age_factor <- relevel(age_factor, ref = 3)) %>%
        mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
        within(gender_factor <- relevel(gender_factor, ref = 2)) 

# cox test for general recidivism
pro.cox.gen <- Surv(start, end, event.gen, type="counting") ~ score_factor
model.pro.cox.gen <- coxph(pro.cox.gen, data=cox.test)
summary(model.pro.cox.gen)

# cox test for violent recidivism
pro.cox.vio <- Surv(start, end, event.vio, type="counting") ~ v_score_factor
model.pro.cox.vio <- coxph(pro.cox.vio, data=cox.test)
summary(model.pro.cox.vio)

```

> The metrics for benchmarking are as follows:

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|General Recidivism| `r 100* round(model.pro.cox.gen[["concordance"]][["concordance"]],3)`%| `r 100* round( a[["overall"]][["Accuracy"]],3)`%|`r 100* round(a[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(a[["byClass"]][["Specificity"]],3)`%|
|Violent Recidivism| `r 100* round(model.pro.cox.vio[["concordance"]][["concordance"]],3)`%| `r 100* round( b[["overall"]][["Accuracy"]],3)`%|`r 100* round(b[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(b[["byClass"]][["Specificity"]],3)`%|



# C. Task 1 & 4: Finding a better model for general recidivism 

**NOTE: general recidivism is run on the original training set (without any synthetic data)**

**The model is being tested on the original test set (without any synthetic data)**

## C.1. LDA Analysis (General recidivism)

#### General LDA Model 
```{r, cache = TRUE}
#--------------------------------------GENERAL LDA MODEL--------------------------------------
nonviolent.lda <- lda(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

#nonviolent.lda

# Training Accuracy 
nonviolent.lda.train <- predict(nonviolent.lda, type="response")
c.1 <- confusionMatrix(as.factor(nonviolent.lda.train$class),as.factor(train$is_recid),positive = "1")
c.1

# Test the Nonviolent LDA model
nonviolent.lda.test = predict(nonviolent.lda, newdata=test)
c <- confusionMatrix(as.factor(nonviolent.lda.test$class),as.factor(test$is_recid),positive = "1")
c
```

#### General LDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels <- test 
test_with_labels$nonviolent_lda_labels = nonviolent.lda.test$class
test_with_labels.lda.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_lda_labels)

# Write lda predictions to cox-test 
cox.test.gen.lda <- inner_join(cox.test,test_with_labels.lda.gen,by="id") 
cox.test.gen.lda <- cox.test.gen.lda %>%
  mutate(gen_score_factor_lda = factor(nonviolent_lda_labels)) %>%    
  within(gen_score_factor_lda <- relevel(gen_score_factor_lda, ref = 1))

lda.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_lda
gen_lda_model <- coxph(lda.cox.gen, data=cox.test.gen.lda)
#summary(gen_lda_model)

```

> General LDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training|  |`r 100* round(c.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(c.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(gen_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(c[["overall"]][["Accuracy"]],3)`%|`r 100* round(c[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **LDA** model performed better in the test set on the accuracy and specificity.  

## C.2 QDA Analysis (General recidivism)

#### General QDA Model
```{r, cache = TRUE}
#--------------------------------------GENERAL QDA MODEL--------------------------------------
nonviolent.qda <- qda(is_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

#nonviolent.qda

# Training Accuracy 
nonviolent.qda.train = predict(nonviolent.qda, type="response")
d.1 <- confusionMatrix(as.factor(nonviolent.qda.train$class),as.factor(train$is_recid),positive = "1")
d.1

# Test the Nonviolent QDA model
nonviolent.qda.test = predict(nonviolent.qda, newdata=test)
d <- confusionMatrix(as.factor(nonviolent.qda.test$class),as.factor(test$is_recid),positive = "1")
d

```

#### General QDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_qda_labels = nonviolent.qda.test$class
test_with_labels.qda.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_qda_labels)

# Write qda predictions to cox-test 
cox.test.gen.qda <- inner_join(cox.test,test_with_labels.qda.gen,by="id") 
cox.test.gen.qda <- cox.test.gen.qda %>%
  mutate(gen_score_factor_qda = factor(nonviolent_qda_labels)) %>%    
  within(gen_score_factor_qda <- relevel(gen_score_factor_qda, ref = 1))

qda.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_qda
gen_qda_model <- coxph(qda.cox.gen, data=cox.test.gen.qda)
#summary(gen_qda_model)

```

> General QDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(d.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(d.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( d[["overall"]][["Accuracy"]],3)`%|`r 100* round(d[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **QDA** model performed better in the test set on specificity. 

## C.3 Decision Tree (General recidivism)

#### General Decision Tree Model
```{r,fig.height=6}
#-----------------------------------------------GENERAL DECISION TREE-----------------------------------------------

dt_object_gen <- rpart(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)


printcp(dt_object_gen)
dt_object_gen.party <- as.party(dt_object_gen)
plot(dt_object_gen.party)
print(dt_object_gen.party)
#plotcp(dt_object_gen)

# Training Accuracy
dt_pred_train_gen <- predict(dt_object_gen, type="class" )
e.1 <- confusionMatrix(as.factor(dt_pred_train_gen),as.factor(train$is_recid),positive = "1")
e.1

# Test Accuracy
dt_pred_test_gen <- predict(dt_object_gen, newdata = test, type="class" )
e <- confusionMatrix(as.factor(dt_pred_test_gen),as.factor(test$is_recid),positive = "1")
e
```

#### General Decision Tree Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_dt_labels = dt_pred_test_gen
test_with_labels.dt.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_dt_labels)

# Write dt predictions to cox-test 
cox.test.gen.dt <- inner_join(cox.test,test_with_labels.dt.gen,by="id") 
cox.test.gen.dt <- cox.test.gen.dt %>%
  mutate(gen_score_factor_dt = factor(nonviolent_dt_labels)) %>%    
  within(gen_score_factor_dt <- relevel(gen_score_factor_dt, ref = 1))

dt.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_dt
gen_dt_model <- coxph(dt.cox.gen, data=cox.test.gen.dt)
#summary(gen_dt_model)

```

> General Decision Tree: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( e.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(e.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( e[["overall"]][["Accuracy"]],3)`%|`r 100* round(e[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **decision tree** model perform better on the test set, though there is a decrease in sensitivity. The predictors used for splitting are number of prior crimes, age and gender   

## C.4 Random Forest (General recidivism)

#### General Random Forest Model
```{r}
#-----------------------------------------------GENERAL RANDOM FOREST-----------------------------------------------
set.seed(10000)
rf_object_gen <- randomForest(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count,data=train,importance=TRUE)

#print(rf_object_gen)
#varImpPlot(rf_object_gen)

# Training Accuracy
rf_pred_train_gen <- predict(rf_object_gen,
                   type="class" )
f.1 <- confusionMatrix(as.factor(rf_pred_train_gen),as.factor(train$is_recid),positive = "1")
f.1

# Test Accuracy
rf_pred_test_gen <- predict(rf_object_gen,
                   newdata = test,
                   type="class")
f <- confusionMatrix(as.factor(rf_pred_test_gen),as.factor(test$is_recid),positive = "1")
f

```

#### General Random Forest Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_rf_labels = rf_pred_test_gen
test_with_labels.rf.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_rf_labels)

# Write rf predictions to cox-test 
cox.test.gen.rf <- inner_join(cox.test,test_with_labels.rf.gen,by="id") 
cox.test.gen.rf <- cox.test.gen.rf %>%
  mutate(gen_score_factor_rf = factor(nonviolent_rf_labels)) %>%    
  within(gen_score_factor_rf <- relevel(gen_score_factor_rf, ref = 1))

rf.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_rf
gen_rf_model <- coxph(rf.cox.gen, data=cox.test.gen.rf)
#summary(gen_rf_model)
```

> General Random Forest: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( f.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(f.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( f[["overall"]][["Accuracy"]],3)`%|`r 100* round(f[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **Random Forest** model performed better in the test set on the accuracy, sensitivity, and specificity.    

## C.5 Logistic Regression (General recidivism)

#### General Logistic Regression Model
```{r, cache = TRUE}
#--------------------------------------GENERAL LOGISTIC REGRESSION MODEL--------------------------------------
nonviolent.log <- glm(is_recid ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count, family="binomial", data=train)


#summary(nonviolent.log)
#varImp(nonviolent.log)

# Training Accuracy
log_pred_train_gen <- predict(nonviolent.log,type="response")
log_pred_train_gen.pred <- rep(0, length(log_pred_train_gen ))
log_pred_train_gen.pred[log_pred_train_gen  > 0.5] <- 1
g.1 <- confusionMatrix(as.factor(log_pred_train_gen.pred),as.factor(train$is_recid),positive = "1")
g.1

# Test Accuracy
log_pred_test_gen <- predict(nonviolent.log,newdata = test,type="response")
log_pred_test_gen.pred <- rep(0, length(log_pred_test_gen))
log_pred_test_gen.pred[log_pred_test_gen > 0.5] <- 1
g <- confusionMatrix(as.factor(log_pred_test_gen.pred),as.factor(test$is_recid),positive = "1")
g

```

#### General Logisitc Regression Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_log_labels = log_pred_test_gen.pred
test_with_labels.log.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_log_labels)

# Write log predictions to cox-test 
cox.test.gen.log <- inner_join(cox.test,test_with_labels.log.gen,by="id") 
cox.test.gen.log <- cox.test.gen.log %>%
  mutate(gen_score_factor_log = factor(nonviolent_log_labels)) %>%    
  within(gen_score_factor_log<- relevel(gen_score_factor_log, ref = 1))

log.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_log
gen_log_model <- coxph(log.cox.gen, data=cox.test.gen.log)
#summary(gen_log_model)

```

> General Logistics Regression: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( g.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(g.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(gen_log_model[["concordance"]][["concordance"]],3)`%   |`r 100* round( g[["overall"]][["Accuracy"]],3)`%|`r 100* round(g[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **Logistics Regression** model performed better in the test set on the accuracy and specificity.

## C.6 KNN (General recidivism)

#### General KNN Model

To find the optimal K in KNN, we will use 10-fold cross-validation.

```{r, cache = TRUE}
#--------------------------------------General KNN MODEL--------------------------------------
# To find the best K 
ctrl <- trainControl(method = "cv", number = 10, 
                     savePredictions = TRUE)

gen.knn <- train(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + juv_fel_count + juv_misd_count + juv_other_count,  data=train, method="knn", trControl = ctrl, tuneLength = 100)

#gen.knn
#varImp(gen.knn)
plot(gen.knn)

# Training Accuracy
knn_pred_train_gen <- predict(gen.knn)
h.1 <- confusionMatrix(as.factor(knn_pred_train_gen),as.factor(train$is_recid),positive = "1")
h.1

# Test Accuracy
knn_pred_test_gen <- predict(gen.knn, newdata = test)
h <-  confusionMatrix(as.factor(knn_pred_test_gen),as.factor(test$is_recid),positive = "1")
h

```

#### General KNN Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_knn_labels = knn_pred_test_gen
test_with_labels.knn.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_knn_labels)

# Write knn predictions to cox-test 
cox.test.gen.knn <- inner_join(cox.test,test_with_labels.knn.gen,by="id") 
cox.test.gen.knn <- cox.test.gen.knn %>%
  mutate(gen_score_factor_knn = factor(nonviolent_knn_labels)) %>%    
  within(gen_score_factor_knn<- relevel(gen_score_factor_knn, ref = 1))

knn.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_knn
gen_knn_model <- coxph(knn.cox.gen, data=cox.test.gen.knn)
#summary(gen_knn_model)

```

> General KNN: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(h.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(h.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h.1[["byClass"]][["Specificity"]],3)`%|
|Test|  `r 100* round(gen_knn_model[["concordance"]][["concordance"]],3)`%        |`r 100* round(h[["overall"]][["Accuracy"]],3)`%|`r 100* round(h[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **KNN** model performed better in the test set on the accuracy, sensitivity, and specificity. The k chosen is `r gen.knn[["bestTune"]][["k"]]`.


# D. Task 2 & 4: Finding a better model for violent recidivism 

**NOTE: violent recidivism is run on the training set that is generated synthetically from the original training set due to balancing issue. Refer to [Part B.5 ](#b.5-checking-for-balance) for more information**

**The model is being tested on the original test set (without any synthetic data)**

## D.1. LDA Analysis (Violent recidivism)

#### Violent LDA Model
```{r, cache = TRUE}
# -------------------------------------VIOLENT LDA MODEL-------------------------------------
violent.lda <- lda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)

#violent.lda

# Training Accuracy
violent.lda.train = predict(violent.lda, type="response")
i.1 <- confusionMatrix(as.factor(violent.lda.train$class),as.factor(train.rose$is_violent_recid),positive = "1")
i.1

# Test the Violent LDA model
violent.lda.test = predict(violent.lda, newdata=test)
i <- confusionMatrix(as.factor(violent.lda.test$class),as.factor(test$is_violent_recid),positive = "1")
i
```

#### Violent LDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_lda_labels = violent.lda.test$class
test_with_labels.lda.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_lda_labels)

# Write lda predictions to cox-test 
cox.test.vio.lda <- inner_join(cox.test,test_with_labels.lda.vio,by="id") 
cox.test.vio.lda <- cox.test.vio.lda %>%
  mutate(vio_score_factor_lda = factor(violent_lda_labels)) %>%    
  within(vio_score_factor_lda <- relevel(vio_score_factor_lda, ref = 1))


lda.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_lda
vio_lda_model <- coxph(lda.cox.vio, data=cox.test.vio.lda)
#summary(vio_lda_model)
```

> Violent LDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( i.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(i.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( i[["overall"]][["Accuracy"]],3)`%|`r 100* round(i[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **LDA** model performed better in the test set on accuracy, sensitivity, and specificity.

## D.2 QDA Analysis (violent recidivism)

#### Violent QDA Model
```{r}
#-----------------------------------------------VIOLENT QDA MODEL------------------------------------------------------------
violent.qda <- qda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)

#violent.qda

# Training Accuracy
violent.qda.train = predict(violent.qda, type="response")
j.1 <- confusionMatrix(as.factor(violent.qda.train$class),as.factor(train.rose$is_violent_recid),positive = "1")
j.1

# Test the Violent QDA model
violent.qda.test = predict(violent.qda, newdata=test)
j <- confusionMatrix(as.factor(violent.qda.test$class),as.factor(test$is_violent_recid),positive = "1")
j

```

#### Violent QDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_qda_labels = violent.lda.test$class
test_with_labels.qda.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_qda_labels)

# Write qda predictions to cox-test 
cox.test.vio.qda <- inner_join(cox.test,test_with_labels.qda.vio,by="id") 
cox.test.vio.qda <- cox.test.vio.qda %>%
  mutate(vio_score_factor_qda = factor(violent_qda_labels)) %>%    
  within(vio_score_factor_qda <- relevel(vio_score_factor_qda, ref = 1))


qda.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_qda
vio_qda_model <- coxph(qda.cox.vio, data=cox.test.vio.qda)
#summary(vio_qda_model)
```

> Violent QDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(j.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(j.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(j[["overall"]][["Accuracy"]],3)`%|`r 100* round(j[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **QDA** model performed better in the test set on accuracy, sensitivity, and specificity but at a huge cost to sensitivity.

## D.3 Decision Tree (Violent recidivism)

#### Violent Decision Tree Model 
```{r,fig.height=7, fig.width=10}
#-----------------------------------------------VIOLENT DECISION TREE-----------------------------------------------

dt_object_vio <- rpart(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)


printcp(dt_object_vio)
dt_object_vio.party <- as.party(dt_object_vio)
plot(dt_object_vio.party)
print(dt_object_vio.party)
#plotcp(dt_object_vio)

# Training Accuracy
dt_pred_train_vio <- predict(dt_object_vio,
                   type="class" )
k.1 <- confusionMatrix(as.factor(dt_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
k.1

# Test Accuracy
dt_pred_test_vio <- predict(dt_object_vio,
                   newdata = test,
                   type="class")
k <- confusionMatrix(as.factor(dt_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
k
```

#### Violent Decision Tree Cox Concordance (Test) 
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_dt_labels = dt_pred_test_vio
test_with_labels.dt.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_dt_labels)

# Write dt predictions to cox-test 
cox.test.vio.dt <- inner_join(cox.test,test_with_labels.dt.vio,by="id") 
cox.test.vio.dt <- cox.test.vio.dt %>%
  mutate(vio_score_factor_dt = factor(violent_dt_labels)) %>%    
  within(vio_score_factor_dt <- relevel(vio_score_factor_dt, ref = 1))


dt.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_dt
vio_dt_model <- coxph(dt.cox.vio, data=cox.test.vio.dt)
#summary(vio_dt_model)


```

> Violent Decision Tree: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( k.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(k.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( k[["overall"]][["Accuracy"]],3)`%|`r 100* round(k[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Decision Tree** model performed better in the test set on accuracy but with lower specificity and sensitivity. The predictors used for splitting are the number of prior crimes, age and gender. This is similar to the trees in the general recidivism. 

Thus, we re-run the training without the juvenile crimes.   
   

## D.4 Random Forest (Violent recidivism)

#### Violent Random Forest Model
```{r}
#-----------------------------------------------VIOLENT RANDOM FOREST-----------------------------------------------
set.seed(10000)
rf_object_vio <- randomForest(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose,importance=TRUE)

#print(rf_object_vio)
#varImpPlot(rf_object_vio)

# Training Accuracy
rf_pred_train_vio <- predict(rf_object_vio,
                   type="class" )
l.1 <- confusionMatrix(as.factor(rf_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
l.1

# Test Accuracy
rf_pred_test_vio <- predict(rf_object_vio,
                   newdata = test,
                   type="class")
l <- confusionMatrix(as.factor(rf_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
l 

```

#### Violent Random Forest Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_rf_labels = rf_pred_test_vio
test_with_labels.rf.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_rf_labels)

# Write rf predictions to cox-test 
cox.test.vio.rf <- inner_join(cox.test,test_with_labels.rf.vio,by="id") 
cox.test.vio.rf <- cox.test.vio.rf %>%
  mutate(vio_score_factor_rf = factor(violent_rf_labels)) %>%    
  within(vio_score_factor_rf <- relevel(vio_score_factor_rf, ref = 1))


rf.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_rf
vio_rf_model <- coxph(rf.cox.vio, data=cox.test.vio.rf)
#summary(vio_rf_model)
```

> Violent Random Forest: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( l.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(l.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(vio_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( l[["overall"]][["Accuracy"]],3)`%|`r 100* round(l[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Random Forest** model performed better in the test set on the accuracy. 

## D.5 Logistic Regression (Violent recidivism)

#### Violent Logistic Regression Model
```{r, cache = TRUE}
#--------------------------------------VIOLENT LOGISTIC REGRESSION MODEL--------------------------------------
violent.log <- glm(is_violent_recid ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count, family="binomial", data=train.rose)


#summary(violent.log)
#varImp(violent.log)

# Training Accuracy
log_pred_train_vio <- predict(violent.log,type="response")
log_pred_train_vio.pred <- rep(0, length(log_pred_train_vio ))
log_pred_train_vio.pred[log_pred_train_vio  > 0.5] <- 1
m.1 <- confusionMatrix(as.factor(log_pred_train_vio.pred),as.factor(train.rose$is_violent_recid),positive = "1")
m.1

# Test Accuracy
log_pred_test_vio <- predict(violent.log,newdata = test,type="response")
log_pred_test_vio.pred <- rep(0, length(log_pred_test_vio))
log_pred_test_vio.pred[log_pred_test_vio > 0.5] <- 1
m <- confusionMatrix(as.factor(log_pred_test_vio.pred),as.factor(test$is_violent_recid),positive = "1")
m

```

#### Violent Logistic Regression Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_log_labels = log_pred_test_vio.pred
test_with_labels.log.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_log_labels)

# Write log predictions to cox-test 
cox.test.vio.log <- inner_join(cox.test,test_with_labels.log.vio,by="id") 
cox.test.vio.log <- cox.test.vio.log %>%
  mutate(vio_score_factor_log = factor(violent_log_labels)) %>%    
  within(vio_score_factor_log <- relevel(vio_score_factor_log, ref = 1))

log.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_log
vio_log_model <- coxph(log.cox.vio, data=cox.test.vio.log)
#summary(vio_log_model)
```

> Violent Logisitcs Regression: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( m.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(m.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_log_model[["concordance"]][["concordance"]],3)`%|`r 100* round( m[["overall"]][["Accuracy"]],3)`%|`r 100* round(m[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Logistics Regression** model performed better in the test set on the accuracy,sensitivity and specificity. 

## D.6 KNN (Violent recidivism)

#### Violent KNN Model

To find the optimal K in KNN, we will use 10-fold cross-validation

```{r, cache = TRUE}
#--------------------------------------VIOLENT KNN MODEL--------------------------------------
# To find the best K 
ctrl <- trainControl(method = "cv", number = 10, 
                     savePredictions = TRUE)

violent.knn <- train(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count,  data=train.rose, method="knn", trControl = ctrl, tuneLength = 100)

#violent.knn
#varImp(violent.knn)
plot(violent.knn)

# Training Accuracy
knn_pred_train_vio <- predict(violent.knn)
n.1 <- confusionMatrix(as.factor(knn_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
n.1

# Test Accuracy
knn_pred_test_vio <- predict(violent.knn, newdata = test)
n <-  confusionMatrix(as.factor(knn_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
n

```

#### Violent KNN Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_knn_labels = knn_pred_test_vio
test_with_labels.knn.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_knn_labels)

# Write knn predictions to cox-test 
cox.test.vio.knn <- inner_join(cox.test,test_with_labels.knn.vio,by="id") 
cox.test.vio.knn <- cox.test.vio.knn %>%
  mutate(vio_score_factor_knn = factor(violent_knn_labels)) %>%    
  within(vio_score_factor_knn <- relevel(vio_score_factor_knn, ref = 1))

knn.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_knn
vio_knn_model <- coxph(knn.cox.vio, data=cox.test.vio.knn)
#summary(vio_knn_model)

```


> Violent KNN: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(n.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(n.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n.1[["byClass"]][["Specificity"]],3)`%| 
|Test|`r 100* round(vio_knn_model[["concordance"]][["concordance"]],3)`%  |`r 100* round(n[["overall"]][["Accuracy"]],3)`%|`r 100* round(n[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n[["byClass"]][["Specificity"]],3)`%| 

For violent recidivism, the **KNN** model performed worse in the test set on the accuracy,sensitivity, and specificity. The k chosen is `r violent.knn[["bestTune"]][["k"]]`.

# E. Task 1 & 2 & 4 Choosing the Best Model

> General Recidivism (Test Set) 

| Model    |  Accuracy | Sensitivity | Specificity| Cox Concordance  |
|----------|-------|----------|-------------|---------------------------------|
|Propublica| `r 100* round( a[["overall"]][["Accuracy"]],3)`%|`r 100* round(a[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(a[["byClass"]][["Specificity"]],3)`%|`r 100* round(model.pro.cox.gen[["concordance"]][["concordance"]],3)`%|
|LDA| `r 100* round(c[["overall"]][["Accuracy"]],3)`%|`r 100* round(c[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_lda_model[["concordance"]][["concordance"]],3)`%|
|QDA| `r 100* round( d[["overall"]][["Accuracy"]],3)`%|`r 100* round(d[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_qda_model[["concordance"]][["concordance"]],3)`%|
|Decision Tree| `r 100* round( e[["overall"]][["Accuracy"]],3)`%|`r 100* round(e[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_dt_model[["concordance"]][["concordance"]],3)`%|
|Random Forest| `r 100* round( f[["overall"]][["Accuracy"]],3)`%|`r 100* round(f[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_rf_model[["concordance"]][["concordance"]],3)`%|
|Logistic Regression| `r 100* round( g[["overall"]][["Accuracy"]],3)`%|`r 100* round(g[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_log_model[["concordance"]][["concordance"]],3)`%   |
|KNN| `r 100* round(h[["overall"]][["Accuracy"]],3)`%|`r 100* round(h[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h[["byClass"]][["Specificity"]],3)`%|`r 100* round(gen_knn_model[["concordance"]][["concordance"]],3)`%        |

For the general recidivism test set, we first used accuracy to compare the performance of models. Both random forest and logistic regression have some of the highest accuracy. Then, we looked at sensitivity to determine which of the two models we should conduct further analysis on, or further analyze predictive power across race, age, and gender. We chose to use sensitivity as our second performance metric because in the context of identifying people who are likely to recidivate, we care most about identifying true positives. True positives are people we predict to be high risk for recidivism who do in fact recidivate. Between random forest and logistic regression, random forest has the higher sensitivity. Therefore, we will conduct further analysis for general recidivism with the **random forest model**.


> Violent Recidivism (Test Set) 

| Model    |  Accuracy | Sensitivity | Specificity| Cox Concordance    |
|----------|-------|----------|-------------|---------------------------------|
|Propublica|  `r 100* round( b[["overall"]][["Accuracy"]],3)`%|`r 100* round(b[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(b[["byClass"]][["Specificity"]],3)`%|`r 100* round(model.pro.cox.vio[["concordance"]][["concordance"]],3)`%|
|LDA| `r 100* round( i[["overall"]][["Accuracy"]],3)`%|`r 100* round(i[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i[["byClass"]][["Specificity"]],3)`%|`r 100* round(vio_lda_model[["concordance"]][["concordance"]],3)`%|
|QDA| `r 100* round(j[["overall"]][["Accuracy"]],3)`%|`r 100* round(j[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j[["byClass"]][["Specificity"]],3)`%|`r 100* round(vio_qda_model[["concordance"]][["concordance"]],3)`%|
|Decision Tree| `r 100* round( k[["overall"]][["Accuracy"]],3)`%|`r 100* round(k[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k[["byClass"]][["Specificity"]],3)`%|`r 100* round(vio_dt_model[["concordance"]][["concordance"]],3)`%|
|Random Forest|`r 100* round( l[["overall"]][["Accuracy"]],3)`%|`r 100* round(l[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l[["byClass"]][["Specificity"]],3)`%| `r 100* round(vio_rf_model[["concordance"]][["concordance"]],3)`%|
|Logistic Regression|`r 100* round( m[["overall"]][["Accuracy"]],3)`%|`r 100* round(m[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m[["byClass"]][["Specificity"]],3)`%|`r 100* round(vio_log_model[["concordance"]][["concordance"]],3)`%|
|KNN|`r 100* round(n[["overall"]][["Accuracy"]],3)`%|`r 100* round(n[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n[["byClass"]][["Specificity"]],3)`%|`r 100* round(vio_knn_model[["concordance"]][["concordance"]],3)`%     |


For the violent recidivism test set, we again used accuracy as our first performance metric. The two models with the highest accuracy are random forests and QDA. However, QDA has a lower sensitivity than random forest. Therefore, we will conduct further analysis for violent recidivism with the **random forest** model as well.

# F. Task 1 & 2 Most Important Predictors

The most important predictors for general recidvism are age, race, number of prior crimes, and number of juvenile crimes (other). 

The most important predictors for violent recidvism are age, race, number of prior crimes, and gender.

## F.1 General Recidivism

### F.1.1 LDA (General Recidivism)
```{r}
lda.1 <- data.frame(nonviolent.lda$scaling)
lda.1 <- lda.1 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(abs(LD1)))
lda.1
```

> The larger the coefficient of the linear discriminant, the more impact the predictors has over the LDA decision rule. When we look at the absolute values of the coefficients, age and race are the most important predictors for **LDA General Recidivism**.

### F.1.2 QDA (General Recidivism)

> There is no coefficients for QDA. Thus, we cannot identify the important predictors.

### F.1.3 Decision Tree (General Recidivism)

```{r}
dt.1 <- varImp(dt_object_gen) 
dt.1 <- dt.1 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
dt.1
```

> Number of prior crimes, age and number of juvenile crimes (other) are the most important predictors for **Decision Tree General Recidivism**.

### F.1.4 Random Forest (General Recidivism)

```{r}
varImpPlot(rf_object_gen) 
```

> Number of prior crimes, age, number of juvenile crimes (other), and race are the most important predictors for **Random Forest General Recidivism**.

### F.1.5 Logistic Regression (General Recidivism)

```{r}
log.1 <- varImp(nonviolent.log) 
log.1 <- log.1 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
log.1
```

> Number of prior crimes, and age are the most important predictors for **Logistic Regression General Recidivism**.

### F.1.6 KNN (General Recidivism)

```{r}
knn.1 <- varImp(gen.knn)$importance 
knn.1 <- knn.1 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
knn.1
```

> Number of prior crimes, age, and juvenile crimes (other) are the most important predictors for **KNN General Recidivism**.


## F.2 Violent Recidivism

### F.2.1 LDA (Violent Recidivism)
```{r}
lda.2 <- data.frame(violent.lda$scaling)
lda.2 <- lda.2 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(abs(LD1)))
lda.2
```

> The larger the coefficient of the linear discriminant, the more impact the predictors has over the LDA decision rule. When we look at the absolute values of the coefficients, age, race and gender are the most important predictors for **LDA Violent Recidivism**.

### F.2.2 QDA (Violent Recidivism)

> There is no coefficients for QDA. Thus, we cannot identify the important predictors.

### F.2.3 Decision Tree (Violent Recidivism)

```{r}
dt.2 <- varImp(dt_object_vio) 
dt.2 <- dt.2 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
dt.2
```

> Race, age, number of prior crimes are the most important predictors for **Decision Tree Violent Recidivism**.

### F.2.4 Random Forest (Violent Recidivism)

```{r}
varImpPlot(rf_object_vio) 
```

> Number of prior crimes, race, and age are the most important predictors for **Random Forest Violent Recidivism**.

### F.2.5 Logistic Regression (Violent Recidivism)

```{r}
log.2 <- varImp(violent.log) 
log.2 <- log.2 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
log.2
```

> Age, gender, and number of prior crimes are the most important predictors for **Logistic Regression Violent Recidivism**.

### F.2.6 KNN (violent Recidivism)

```{r}
knn.2 <- varImp(violent.knn)$importance 
knn.2 <- knn.2 %>% 
  rownames_to_column("predictors") %>%
  arrange(desc(Overall))
knn.2
```

> Number of prior crimes, gender, and age are the most important predictors for **KNN Violent Recidivism**.

# G. Task 3 Looking at trends across sub-groups

We will be looking at determining whether random forests are equally predictive across race, age, and gender for general and violent recidivism.

## G.1 Survival Plot General Recidivism Random Forest
```{r}
fit <- survfit(rf.cox.gen, data=cox.test.gen.rf)
plotty <- function(fit, title) {
  return(autoplot(fit, conf.int=T, censor=F) + ggtitle(title) + ylim(0,1) +
  labs(y = "Survival Rate (no recidivism)"))  }

plotty(fit, "Overall General Recidivism (0 = Low Risk, 1 = High Risk)")
```

> Offenders classified as high risk for **general recidivism** under **random forest** do recidivate at higher rates.

## G.2 Survival Plot Violent Random Forest
```{r}
fit <- survfit(rf.cox.vio, data=cox.test.vio.rf)
plotty(fit, "Overall Violent Recidivism (0 = Low Risk, 1 = High Risk)")

```

> Offenders classified as high risk for **violent recidivism** under **random forest** do violently recidivate at higher rates.

## G.3 Comparing predictive accuracy across demographics (general recidivism) 

Our model are not equally predictive across age and gender as shown by differences in hazard ratios for both general and violent recidivism.

For general and violent recidivism, there is a slight difference in the predictive power of our model across race, though this difference is not significant. 

### G.3.1 Race (general recidivism)

#### Cox Model on race
```{r}
f2 <- Surv(start, end, event.gen, type="counting") ~ race_factor + gen_score_factor_rf + race_factor * gen_score_factor_rf
model2 <- coxph(f2, data=cox.test.gen.rf)
#print(summary(model2))

```

```{r}
print(paste("Black High Hazard:", exp(model2[["coefficients"]][["race_factorAfrican-American:gen_score_factor_rf1"]] + model2[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```

> High-risk black defendants (assigned a label of 1) are approx. `r round(exp(model2[["coefficients"]][["race_factorAfrican-American:gen_score_factor_rf1"]] + model2[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to recidivate than low-risk black defendants (assigned a label of 0).

```{r}
print(paste("White High Hazard:", exp(model2[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```

> High-risk white defendants (assigned a label of 1) are approx. `r round(exp(model2[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to recidivate than low-risk white defendants (assigned a label of 0). 
> Assigning a 1, or a "high risk" label to a white defendant is slightly more indicative of their likelihood of  recidivating than assigning a 1 to a black defendant.

### G.3.2 Age (general recidivism)

#### Cox Model on age
```{r}
f3 <- Surv(start, end, event.gen, type="counting") ~ age_factor + gen_score_factor_rf + age_factor * gen_score_factor_rf
model3 <- coxph(f3, data=cox.test.gen.rf)
#summary(model3)
```

```{r}
print(paste("Over Age 45 High Hazard:", exp(model3[["coefficients"]][["age_factorGreater than 45:gen_score_factor_rf1"]]
 + model3[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```

> High-risk defendants over age 45 are approx. `r round(exp(model3[["coefficients"]][["age_factorGreater than 45:gen_score_factor_rf1"]] + model3[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to  recidivate than low-risk defendants over age 45.

```{r}
print(paste("Under Age 25 High Hazard:", exp(model3[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```

> High-risk defendants under age 25 are approx. `r round(exp( model3[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to  recidivate than low-risk defendants under age 25. 

> Assigning a 1, or a "high risk" label to a person over age 45 is more indicative of their likelihood of recidivating than assigning a 1 to a person under age 25. 

### G.3.3 Gender (general recidivism)

```{r}
f4 <- Surv(start, end, event.gen, type="counting") ~ gender_factor + gen_score_factor_rf + gender_factor * gen_score_factor_rf
model4 <- coxph(f4, data=cox.test.gen.rf)
#print(summary(model4))
```

```{r}
print(paste("Male High Hazard:", exp(model4[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```
> High-risk male defendants are approx. `r round(exp(model4[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to recidivate than low-risk male defendants.

```{r}
print(paste("Female High Hazard:", exp(model4[["coefficients"]][["gender_factorFemale:gen_score_factor_rf1"]] + model4[["coefficients"]][["gen_score_factor_rf1"]]), sep=" "))
```

> High-risk female defendants are approx. `r round(exp(model4[["coefficients"]][["gender_factorFemale:gen_score_factor_rf1"]] + model4[["coefficients"]][["gen_score_factor_rf1"]]),2)` times more likely to  recidivate than low-risk female defendants. 

> Assigning a 1, or a "high risk" label to a female defendant is more indicative of their likelihood of recidivating than assigning a 1 to a male defendant.


## G.4 Comparing predictive accuracy across demographics (violent recidivism) 

### G.4.1 Race (violent recidivism)
```{r}
f5 <- Surv(start, end, event.vio, type="counting") ~ race_factor + vio_score_factor_rf + race_factor * vio_score_factor_rf
model5 <- coxph(f5, data=cox.test.vio.rf)
#print(summary(model5))
```

```{r}
print(paste("Black High Hazard:", exp(model5[["coefficients"]][["race_factorAfrican-American:vio_score_factor_rf1"]] + model5[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```
> High-risk black defendants (assigned a label of 1) are approx. `r round(exp(model5[["coefficients"]][["race_factorAfrican-American:vio_score_factor_rf1"]] + model5[["coefficients"]][["vio_score_factor_rf1"]]),2)` times more likely to violently recidivate than low-risk black defendants (assigned a label of 0).

```{r}
print(paste("White High Hazard:", exp(model5[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```

> High-risk white defendants (assigned a label of 1) are approx. `r round(exp(model5[["coefficients"]][["vio_score_factor_rf1"]]),2)` times more likely to violently recidivate than low-risk white defendants (assigned a label of 0).

> Assigning a 1, or a "high risk" label to a black defendant is more indicative of their likelihood of violently recidivating as compared to assigning a 1 to a white defendant.

### G.4.2 Age (violent recidivism)
```{r}
f6 <- Surv(start, end, event.vio, type="counting") ~ age_factor + vio_score_factor_rf + age_factor * vio_score_factor_rf
model6 <- coxph(f6, data=cox.test.vio.rf)
#summary(model6)
```

```{r}
print(paste("Over Age 45 High Hazard:", exp(model6[["coefficients"]][["age_factorGreater than 45:vio_score_factor_rf1"]]
 + model6[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```
> High-risk defendants over age 45 are approx. `r round(exp(model6[["coefficients"]][["age_factorGreater than 45:vio_score_factor_rf1"]]
 + model6[["coefficients"]][["vio_score_factor_rf1"]]),2)` times more likely to violently recidivate than low-risk defendants over age 45.

```{r}
print(paste("Under Age 25 High Hazard:", exp(model6[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```

> High-risk defendants under age 25 are approx. `r round(exp( model6[["coefficients"]][["vio_score_factor_rf1"]]),2)`  times more likely to violently recidivate than low-risk defendants under age 25. 

> Assigning a 1, or a "high risk" label to a defendant under age 25 is more indicative of their likelihood of violently recidivating than assigning a 1 to a defendant on over age 45.

### G.4.3 Gender (violent recidivism)

```{r}  
f7 <- Surv(start, end, event.vio, type="counting") ~ gender_factor + vio_score_factor_rf + gender_factor * vio_score_factor_rf
model7 <- coxph(f7, data=cox.test.vio.rf)
#print(summary(model7))
```

```{r}  
print(paste("Male High Hazard:", exp( model7[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```
> High-risk male defendants are approx. `r round(exp( model7[["coefficients"]][["vio_score_factor_rf1"]]),3)` times more likely to violently recidivate than low-risk male defendants.

```{r}  
print(paste("Female High Hazard:", exp(model7[["coefficients"]][["gender_factorFemale:vio_score_factor_rf1"]] + model7[["coefficients"]][["vio_score_factor_rf1"]]), sep=" "))
```
> High-risk female defendants are approx. `r round(exp(model7[["coefficients"]][["gender_factorFemale:vio_score_factor_rf1"]] + model7[["coefficients"]][["vio_score_factor_rf1"]]),2)` times more likely to violently recidivate than low-risk female defendants.

> Assigning a 1, or a "high risk" label to a male defendant is more indicative of their likelihood of violently recidivating than assigning a 1 to a female defendant.




