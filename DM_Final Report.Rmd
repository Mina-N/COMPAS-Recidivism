---
title: "DM Final Report"
author: "Yang Le Lim & Mina Narayanan & Samvrudhi Shankar"
date: "5/12/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
---

```{r setup, include=FALSE}
# load library 
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(randomForest)
library(tidyverse)
library(ggplot2)
library(ISLR)
library(MASS)
library(klaR) 
library(knitr)
library(glmnet)
library(gam)
library(plyr)
library(reshape)
library(boot)
library(survival)
library(ggfortify)
library(caret)
library(pROC)
library(ROSE)
library(tree)
library(partykit)
library(gridExtra)
options(scipen = 4)  # Suppresses scientific notation
```

# A. Summary 

For this data mining project, we are looking at criminal recidivism prediction of done by COMPAS and Propublica.

Our analysis seeks to

1. Construct an RAI for predicting two-year recidivism on the Broward County population

    - Select the most salient predictors from **compas-scores-two-years.csv** to estimate the outcome variable **is_recid**

    - Split the data into test (40% of the data) and training (60% of the data) datasets and record training error

    - Revise the predictors in the model as needed

2. Construct an RAI for predicting two-year violent recidivism on the Broward County population

    - Select the most salient predictors from **compas-scores-two-years.csv** to estimate the outcome variable **is_violent_recid**

    - Split the data into test (40% of the data) and training (60% of the data) datasets and record training error

    - Revise the predictors in the model as needed

3. Determine whether each RAI is equally predictive across race, age, and gender

    - Compare performance of Random Forests across race, age, and gender

4. Compare the performance of our RAIs to COMPAS

     - Cox Proportional Hazards Model
     
     - Confusion Matrix 

# B. Preparation

## B.1 Read in compas data
```{r}
# Read in compas data
raw_data <- read.csv("./compas-scores-two-years.csv")
nrow(raw_data)

compas_non_violent <- raw_data %>% 
  filter(days_b_screening_arrest <= 30) %>%
  filter(days_b_screening_arrest >= -30) %>%
  filter(is_recid != -1) %>%
  filter(c_charge_degree != "O") %>%
  filter(score_text != 'N/A')
nrow(compas_non_violent)

compas_non_violent <- mutate(compas_non_violent, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor.1 = factor(score_text != "Low", labels = c("LowScore","HighScore"))) %>%
      mutate(v_score_factor.1 = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))

# Read in cox proportional hazards data
cox_parsed <- read.csv("./cox-parsed.csv")
```

## B.2 Create train and test datasets
```{r}
set.seed(1)
index = sample( 1:nrow( compas_non_violent ), round( nrow( compas_non_violent )*0.6 ), replace = FALSE )
train <- compas_non_violent[ index, ] %>% # About 60% of the observations
  arrange(id) 
test <- compas_non_violent[ -index, ] %>% # About 40% of the observations
  arrange(id) 
```

## B.3 Modelling  

We have used LDA, QDA, decision tree, random forest, logistic regression,and KNN to find a better model.

The models were first run on the training set before gathering the various metrics on the test set.

There are two outcomes of interest:
  
  1) general recidivism, which includes both non-violent and violent
  
  2) violent recidivism. 
  
The predictors in the dataset which can be used for a pre-arrest assessment are gender, age, race, type of crime (misdemeanor or felony), number of prior crimes, number of juvenile crimes committed (misdemeanor, felony, other). 

All these predictors will be used in our modelling. Further adjustments are made based on the output. 

For the violence recidivism, there are still instances of extremely low sensitivity after balancing. The common trend in these cases is that juvenile crimes will become the most important predictors, even though they do not appear important in other models. Thus, we re-trained those model of low sensitivity without juvenile crimes.  

## B.4 Checking for correlation between input variables (continuous)
```{r}
# Check for correlation between input variables
compas.var.names <- c("priors_count", "juv_fel_count", "juv_misd_count", "juv_other_count")
round(cor(compas_non_violent[,compas.var.names]), 3)

# Function to check for correlation 
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = pmax(1, cex.cor * r))
}

pairs(compas_non_violent[,compas.var.names], lower.panel = panel.cor)
```

> There is no obvious correlation between the continuous predictors, which is good.

## B.5 Checking for balance

When we did a preliminary run of our models on violent recidivism, it cannot perform very well as the proportion of the majority class is too large (no violent recidivism). They tend to predict every observation as the majority class, as this can maximize accuracy more easily. This is at the cost of sensitivity as the actual violent recidivism is not predicted correctly.

To compensate for the lack of balance, we decided to use Synthetic Data Generation to create more observations for the minority class using the existing predictors. This can improve the performance of the model while retaining the distribution of the underlying dataset. See the source below for more information.

Source: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/

```{r}
# is_violent_recid is unbalanced
table(train$is_recid) # train general recidivisim (num)
prop.table(table(train$is_recid)) # train general recidivisim (proportion)
table(test$is_recid) # test general recidivisim (num)
prop.table(table(test$is_recid)) # test general recidivisim (proportion)
table(train$is_violent_recid) # train violent recidivisim (num)
prop.table(table(train$is_violent_recid)) # train violent recidivisim (proportion)
table(test$is_violent_recid)  # test violent recidivisim (num)
prop.table(table(test$is_violent_recid)) # test violent recidivisim (proportion)
```

#### Example of a model on violent recidivism in the training set
```{r}
violent.lda <- lda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

# Training Accuracy 
violent.lda.train = predict(violent.lda, type="response")
confusionMatrix(as.factor(violent.lda.train$class),as.factor(train$is_violent_recid),positive = "1")

# Testing Accuracy
violent.lda.test = predict(violent.lda, newdata=test)
confusionMatrix(as.factor(violent.lda.test$class),as.factor(test$is_violent_recid),positive = "1")
```

> As seen in the example above, sensitivity is very low (close to zero), even though there is high accuracy. The proportion of actual violent recidivism being correctly classified is low.  

#### Synthetic Data Generation
```{r, fig.width=11}
# to ensure synthetic data for continuous variables remains as an integer
train.1 <- train %>%
      mutate(priors_count_factor = factor(priors_count)) %>%
      mutate(juv_fel_count_factor = factor(juv_fel_count)) %>%
      mutate(juv_misd_count_factor = factor(juv_misd_count)) %>%
      mutate(juv_other_count_factor = factor(juv_other_count)) 

train.rose <- ROSE(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count_factor + juv_fel_count_factor + juv_misd_count_factor + juv_other_count_factor, data = train.1, seed = 1)$data

# reconvert the continuous variables back to a number type
train.rose <- train.rose %>%
  mutate(priors_count = as.numeric(as.character(priors_count_factor))) %>%
      mutate(juv_fel_count = as.numeric(as.character(juv_fel_count_factor))) %>%
      mutate(juv_misd_count = as.numeric(as.character(juv_misd_count_factor))) %>%
      mutate(juv_other_count = as.numeric(as.character(juv_other_count_factor))) 

table(train.rose$is_violent_recid) # train violent recidivisim with balancing (num)
prop.table(table(train.rose$is_violent_recid)) # train violent recidivisim with balancing (proportion)

# plotting the distribution of prior crime count 
plot.1 <- train.1 %>%
  filter(is_violent_recid==0) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Actual train set for prior crime (no violent recid)")

plot.2  <- train.1 %>%
  filter(is_violent_recid==1) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Actual train set for prior crime (violent recid)")

plot.3 <- train.rose %>%
  filter(is_violent_recid==0) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Synthetic train set for prior crime (no violent recid)")

plot.4 <- train.rose %>%
  filter(is_violent_recid==1) %>%
  ggplot(aes(x=priors_count)) +
  geom_histogram() +
  facet_wrap(~is_violent_recid)+
  labs(title = "Synthetic train set for prior crime (violent recid)")

grid.arrange(plot.1,plot.3, ncol=2)
grid.arrange(plot.2,plot.4, ncol=2)

```

>  After balancing, we now have similar proportions of outcomes in training set for violent recidivism. From the histogram of prior crimes, we can see that the synthetic data has similar distributions with the actual data.


## B.6 Creating Benchmark 

In the study, the Cox Model uses concordance as an indicator of accuracy. To create a comparable benchmark with Propublica, the csv input of Propublica's code will be restricted to observations that appear in our test set.

We will also use a confusion matrix on the test set as another benchmark.

#### Confusion matrix of the test set 
```{r}
# Revaluing the levels of low score to zero and high score to 1
test$score_factor.1 <- revalue(test$score_factor.1, c("LowScore" = 0, "HighScore" = 1))
test$v_score_factor.1 <- revalue(test$v_score_factor.1, c("LowScore" = 0, "HighScore" = 1))

# General Recidivism
a <- confusionMatrix(as.factor(test$score_factor.1),as.factor(test$is_recid),positive = "1")
a

# Violent Recidivism
b <- confusionMatrix(as.factor(test$v_score_factor.1),as.factor(test$is_violent_recid),positive = "1")
b

```

#### Adapting Propublica's Cox Model code to create benchmark
```{r}
# obtain subset of csv input of Propublica's code
cox.test <- semi_join(cox_parsed,test,by="id")
cox.test <- filter(filter(cox.test, score_text != "N/A", v_score_text != "N/A"), end > start) %>%
        mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>% 
        within(race_factor <- relevel(race_factor, ref = 3)) %>% # set the reference group
        mutate(score_factor = factor(score_text)) %>%
        within(score_factor <- relevel(score_factor, ref=2)) %>%
        mutate(v_score_factor = factor(v_score_text)) %>%
        within(v_score_factor <- relevel(v_score_factor, ref=2)) %>%
        mutate(event.gen = event) %>%
        mutate(event.vio = if_else(event.gen == 0, as.integer(0), is_violent_recid))

# cox test for general recidivism
pro.cox.gen <- Surv(start, end, event.gen, type="counting") ~ score_factor
model.pro.cox.gen <- coxph(pro.cox.gen, data=cox.test)
summary(model.pro.cox.gen)

# cox test for violent recidivism
pro.cox.vio <- Surv(start, end, event.vio, type="counting") ~ v_score_factor
model.pro.cox.vio <- coxph(pro.cox.vio, data=cox.test)
summary(model.pro.cox.vio)

```

> The metrics for benchmarking are as follows:

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|General Recidivism| `r 100* round(model.pro.cox.gen[["concordance"]][["concordance"]],3)`%| `r 100* round( a[["overall"]][["Accuracy"]],3)`%|`r 100* round(a[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(a[["byClass"]][["Specificity"]],3)`%|
|Violent Recidivism| `r 100* round(model.pro.cox.vio[["concordance"]][["concordance"]],3)`%| `r 100* round( b[["overall"]][["Accuracy"]],3)`%|`r 100* round(b[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(b[["byClass"]][["Specificity"]],3)`%|



# C. Task 1 & 4: Finding a better model for general recidivism 

## C.1. LDA Analysis (General recidivism)

#### General LDA Model 
```{r, cache = TRUE}
#--------------------------------------GENERAL LDA MODEL--------------------------------------
nonviolent.lda <- lda(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

nonviolent.lda

# Training Accuracy 
nonviolent.lda.train <- predict(nonviolent.lda, type="response")
c.1 <- confusionMatrix(as.factor(nonviolent.lda.train$class),as.factor(train$is_recid),positive = "1")
c.1

# Test the Nonviolent LDA model
nonviolent.lda.test = predict(nonviolent.lda, newdata=test)
c <- confusionMatrix(as.factor(nonviolent.lda.test$class),as.factor(test$is_recid),positive = "1")
c
```

#### General LDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels <- test 
test_with_labels$nonviolent_lda_labels = nonviolent.lda.test$class
test_with_labels.lda.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_lda_labels)

# Write lda predictions to cox-test 
cox.test.gen.lda <- inner_join(cox.test,test_with_labels.lda.gen,by="id") 
cox.test.gen.lda <- cox.test.gen.lda %>%
  mutate(gen_score_factor_lda = factor(nonviolent_lda_labels)) %>%    
  within(gen_score_factor_lda <- relevel(gen_score_factor_lda, ref = 1))

lda.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_lda
gen_lda_model <- coxph(lda.cox.gen, data=cox.test.gen.lda)
summary(gen_lda_model)

```

> General LDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training|  |`r 100* round(c.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(c.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(gen_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(c[["overall"]][["Accuracy"]],3)`%|`r 100* round(c[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **LDA** model performed better in the test set on the accuracy and specificity.  

## C.2 QDA Analysis (General recidivism)

#### General QDA Model
```{r, cache = TRUE}
#--------------------------------------GENERAL QDA MODEL--------------------------------------
nonviolent.qda <- qda(is_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)

nonviolent.qda

# Training Accuracy 
nonviolent.qda.train = predict(nonviolent.qda, type="response")
d.1 <- confusionMatrix(as.factor(nonviolent.qda.train$class),as.factor(train$is_recid),positive = "1")
d.1

# Test the Nonviolent QDA model
nonviolent.qda.test = predict(nonviolent.qda, newdata=test)
d <- confusionMatrix(as.factor(nonviolent.qda.test$class),as.factor(test$is_recid),positive = "1")
d

```

#### General QDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_qda_labels = nonviolent.qda.test$class
test_with_labels.qda.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_qda_labels)

# Write qda predictions to cox-test 
cox.test.gen.qda <- inner_join(cox.test,test_with_labels.qda.gen,by="id") 
cox.test.gen.qda <- cox.test.gen.qda %>%
  mutate(gen_score_factor_qda = factor(nonviolent_qda_labels)) %>%    
  within(gen_score_factor_qda <- relevel(gen_score_factor_qda, ref = 1))

qda.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_qda
gen_qda_model <- coxph(qda.cox.gen, data=cox.test.gen.qda)
summary(gen_qda_model)

```

> General QDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(d.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(d.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( d[["overall"]][["Accuracy"]],3)`%|`r 100* round(d[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **QDA** model performed better in the test set on specificity. 

## C.3 Decision Tree (General recidivism)

#### General Decision Tree Model
```{r,fig.height=6}
#-----------------------------------------------GENERAL DECISION TREE-----------------------------------------------

dt_object_gen <- rpart(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train)


printcp(dt_object_gen)
dt_object_gen.party <- as.party(dt_object_gen)
plot(dt_object_gen.party)
print(dt_object_gen.party)
plotcp(dt_object_gen)

# Training Accuracy
dt_pred_train_gen <- predict(dt_object_gen, type="class" )
e.1 <- confusionMatrix(as.factor(dt_pred_train_gen),as.factor(train$is_recid),positive = "1")
e.1

# Test Accuracy
dt_pred_test_gen <- predict(dt_object_gen, newdata = test, type="class" )
e <- confusionMatrix(as.factor(dt_pred_test_gen),as.factor(test$is_recid),positive = "1")
e
```

#### General Decision Tree Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_dt_labels = dt_pred_test_gen
test_with_labels.dt.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_dt_labels)

# Write dt predictions to cox-test 
cox.test.gen.dt <- inner_join(cox.test,test_with_labels.dt.gen,by="id") 
cox.test.gen.dt <- cox.test.gen.dt %>%
  mutate(gen_score_factor_dt = factor(nonviolent_dt_labels)) %>%    
  within(gen_score_factor_dt <- relevel(gen_score_factor_dt, ref = 1))

dt.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_dt
gen_dt_model <- coxph(dt.cox.gen, data=cox.test.gen.dt)
summary(gen_dt_model)

```

> General Decision Tree: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( e.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(e.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( e[["overall"]][["Accuracy"]],3)`%|`r 100* round(e[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **decision tree** model perform better on the test set, though there is a decrease in sensitivity. The predictors used for splitting are number of prior crimes, age and gender   

## C.4 Random Forest (General recidivism)

#### General Random Forest Model
```{r}
#-----------------------------------------------GENERAL RANDOM FOREST-----------------------------------------------
rf_object_gen <- randomForest(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train,importance=TRUE)

print(rf_object_gen)
varImpPlot(rf_object_gen)

# Training Accuracy
rf_pred_train_gen <- predict(rf_object_gen,
                   type="class" )
f.1 <- confusionMatrix(as.factor(rf_pred_train_gen),as.factor(train$is_recid),positive = "1")
f.1

# Test Accuracy
rf_pred_test_gen <- predict(rf_object_gen,
                   newdata = test,
                   type="class")
f <- confusionMatrix(as.factor(rf_pred_test_gen),as.factor(test$is_recid),positive = "1")
f

```

#### General Random Forest Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_rf_labels = rf_pred_test_gen
test_with_labels.rf.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_rf_labels)

# Write rf predictions to cox-test 
cox.test.gen.rf <- inner_join(cox.test,test_with_labels.rf.gen,by="id") 
cox.test.gen.rf <- cox.test.gen.rf %>%
  mutate(gen_score_factor_rf = factor(nonviolent_rf_labels)) %>%    
  within(gen_score_factor_rf <- relevel(gen_score_factor_rf, ref = 1))

rf.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_rf
gen_rf_model <- coxph(rf.cox.gen, data=cox.test.gen.rf)
summary(gen_rf_model)
```

> General Random Forest: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( f.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(f.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(gen_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( f[["overall"]][["Accuracy"]],3)`%|`r 100* round(f[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **Random Forest** model performed better in the test set on the accuracy, sensitivity, and specificity. Similar to the decision tree model, the most important predictors are number of prior crimes, age and gender.   

## C.5 Logistic Regression (General recidivism)

#### General Logistic Regression Model
```{r, cache = TRUE}
#--------------------------------------GENERAL LOGISTIC REGRESSION MODEL--------------------------------------
nonviolent.log <- glm(is_recid ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count, family="binomial", data=train)


summary(nonviolent.log)
varImp(nonviolent.log)

# Training Accuracy
log_pred_train_gen <- predict(nonviolent.log,type="response")
log_pred_train_gen.pred <- rep(0, length(log_pred_train_gen ))
log_pred_train_gen.pred[log_pred_train_gen  > 0.5] <- 1
g.1 <- confusionMatrix(as.factor(log_pred_train_gen.pred),as.factor(train$is_recid),positive = "1")
g.1

# Test Accuracy
log_pred_test_gen <- predict(nonviolent.log,newdata = test,type="response")
log_pred_test_gen.pred <- rep(0, length(log_pred_test_gen))
log_pred_test_gen.pred[log_pred_test_gen > 0.5] <- 1
g <- confusionMatrix(as.factor(log_pred_test_gen.pred),as.factor(test$is_recid),positive = "1")
g

```

#### General Logisitc Regression Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_log_labels = log_pred_test_gen.pred
test_with_labels.log.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_log_labels)

# Write log predictions to cox-test 
cox.test.gen.log <- inner_join(cox.test,test_with_labels.log.gen,by="id") 
cox.test.gen.log <- cox.test.gen.log %>%
  mutate(gen_score_factor_log = factor(nonviolent_log_labels)) %>%    
  within(gen_score_factor_log<- relevel(gen_score_factor_log, ref = 1))

log.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_log
gen_log_model <- coxph(log.cox.gen, data=cox.test.gen.log)
summary(gen_log_model)

```

> General Logistics Regression: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( g.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(g.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(gen_log_model[["concordance"]][["concordance"]],3)`%   |`r 100* round( g[["overall"]][["Accuracy"]],3)`%|`r 100* round(g[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **Logistics Regression** model performed better in the test set on the accuracy and specificity.

## C.6 KNN (General recidivism)

#### General KNN Model

To find the optimal K in KNN, we will use 10-fold cross-validation.

```{r, cache = TRUE}
#--------------------------------------General KNN MODEL--------------------------------------
# To find the best K 
ctrl <- trainControl(method = "cv", number = 10, 
                     savePredictions = TRUE)

gen.knn <- train(as.factor(is_recid) ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + juv_fel_count + juv_misd_count + juv_other_count,  data=train, method="knn", trControl = ctrl, tuneLength = 150)

gen.knn
varImp(gen.knn)
plot(gen.knn)

# Training Accuracy
knn_pred_train_gen <- predict(gen.knn)
h.1 <- confusionMatrix(as.factor(knn_pred_train_gen),as.factor(train$is_recid),positive = "1")
h.1

# Test Accuracy
knn_pred_test_gen <- predict(gen.knn, newdata = test)
h <-  confusionMatrix(as.factor(knn_pred_test_gen),as.factor(test$is_recid),positive = "1")
h

```

#### General KNN Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$nonviolent_knn_labels = knn_pred_test_gen
test_with_labels.knn.gen <- test_with_labels %>%
   dplyr::select(id,name,nonviolent_knn_labels)

# Write knn predictions to cox-test 
cox.test.gen.knn <- inner_join(cox.test,test_with_labels.knn.gen,by="id") 
cox.test.gen.knn <- cox.test.gen.knn %>%
  mutate(gen_score_factor_knn = factor(nonviolent_knn_labels)) %>%    
  within(gen_score_factor_knn<- relevel(gen_score_factor_knn, ref = 1))

knn.cox.gen <- Surv(start, end, event.gen, type="counting") ~ gen_score_factor_knn
gen_knn_model <- coxph(knn.cox.gen, data=cox.test.gen.knn)
summary(gen_knn_model)

```

> General KNN: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(h.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(h.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h.1[["byClass"]][["Specificity"]],3)`%|
|Test|  `r 100* round(gen_knn_model[["concordance"]][["concordance"]],3)`%        |`r 100* round(h[["overall"]][["Accuracy"]],3)`%|`r 100* round(h[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h[["byClass"]][["Specificity"]],3)`%|

For general recidivism, the **KNN** model performed better in the test set on the accuracy and specificity. The k chosen is `r gen.knn[["bestTune"]][["k"]]`.


# D. Task 2 & 4: Finding a better model for violent recidivism 

**NOTE: violent recidivism is run on the training set that is generated synthetically from the original training set due to balancing issue. Refer to [Part B.5 ](#b.5-checking-for-balance) for more information**

## D.1. LDA Analysis (Violent recidivism)

#### Violent LDA Model
```{r, cache = TRUE}
# -------------------------------------VIOLENT LDA MODEL-------------------------------------
violent.lda <- lda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)

violent.lda

# Training Accuracy
violent.lda.train = predict(violent.lda, type="response")
i.1 <- confusionMatrix(as.factor(violent.lda.train$class),as.factor(train.rose$is_violent_recid),positive = "1")
i.1

# Test the Violent LDA model
violent.lda.test = predict(violent.lda, newdata=test)
i <- confusionMatrix(as.factor(violent.lda.test$class),as.factor(test$is_violent_recid),positive = "1")
i
```

#### Violent LDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_lda_labels = violent.lda.test$class
test_with_labels.lda.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_lda_labels)

# Write lda predictions to cox-test 
cox.test.vio.lda <- inner_join(cox.test,test_with_labels.lda.vio,by="id") 
cox.test.vio.lda <- cox.test.vio.lda %>%
  mutate(vio_score_factor_lda = factor(violent_lda_labels)) %>%    
  within(vio_score_factor_lda <- relevel(vio_score_factor_lda, ref = 1))


lda.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_lda
vio_lda_model <- coxph(lda.cox.vio, data=cox.test.vio.lda)
summary(vio_lda_model)
```

> Violent LDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( i.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(i.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( i[["overall"]][["Accuracy"]],3)`%|`r 100* round(i[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **LDA** model performed better in the test set on accuracy and sensitivity.

## D.2 QDA Analysis (violent recidivism)

#### Violent QDA Model
```{r}
#-----------------------------------------------VIOLENT QDA MODEL------------------------------------------------------------
violent.qda <- qda(is_violent_recid ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)

violent.qda

# Training Accuracy
violent.qda.train = predict(violent.qda, type="response")
j.1 <- confusionMatrix(as.factor(violent.qda.train$class),as.factor(train.rose$is_violent_recid),positive = "1")
j.1

# Test the Violent QDA model
violent.qda.test = predict(violent.qda, newdata=test)
j <- confusionMatrix(as.factor(violent.qda.test$class),as.factor(test$is_violent_recid),positive = "1")
j

```

#### Violent QDA Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_qda_labels = violent.lda.test$class
test_with_labels.qda.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_qda_labels)

# Write qda predictions to cox-test 
cox.test.vio.qda <- inner_join(cox.test,test_with_labels.qda.vio,by="id") 
cox.test.vio.qda <- cox.test.vio.qda %>%
  mutate(vio_score_factor_qda = factor(violent_qda_labels)) %>%    
  within(vio_score_factor_qda <- relevel(vio_score_factor_qda, ref = 1))


qda.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_qda
vio_qda_model <- coxph(qda.cox.vio, data=cox.test.vio.qda)
summary(vio_qda_model)
```

> Violent QDA: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(j.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(j.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(j[["overall"]][["Accuracy"]],3)`%|`r 100* round(j[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **QDA** model performed better in the test set on accuracy, sensitivity, and specificity but at a huge cost to sensitivity.

## D.3 Decision Tree (Violent recidivism)

#### Violent Decision Tree Model 
```{r,fig.height=7, fig.width=10}
#-----------------------------------------------VIOLENT DECISION TREE-----------------------------------------------

dt_object_vio <- rpart(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose)


printcp(dt_object_vio)
dt_object_vio.party <- as.party(dt_object_vio)
plot(dt_object_vio.party)
print(dt_object_vio.party)
plotcp(dt_object_vio)

# Training Accuracy
dt_pred_train_vio <- predict(dt_object_vio,
                   type="class" )
k.1 <- confusionMatrix(as.factor(dt_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
k.1

# Test Accuracy
dt_pred_test_vio <- predict(dt_object_vio,
                   newdata = test,
                   type="class")
k <- confusionMatrix(as.factor(dt_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
k
```

#### Violent Decision Tree Cox Concordance (Test) 
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_dt_labels = dt_pred_test_vio
test_with_labels.dt.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_dt_labels)

# Write dt predictions to cox-test 
cox.test.vio.dt <- inner_join(cox.test,test_with_labels.dt.vio,by="id") 
cox.test.vio.dt <- cox.test.vio.dt %>%
  mutate(vio_score_factor_dt = factor(violent_dt_labels)) %>%    
  within(vio_score_factor_dt <- relevel(vio_score_factor_dt, ref = 1))


dt.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_dt
vio_dt_model <- coxph(dt.cox.vio, data=cox.test.vio.dt)
summary(vio_dt_model)


```

> Violent Decision Tree: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( k.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(k.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( k[["overall"]][["Accuracy"]],3)`%|`r 100* round(k[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Decision Tree** model performed better in the test set on accuracy but with lower specificity and sensitivity. The predictors used for splitting are the number of prior crimes, age and gender. This is similar to the trees in the general recidivism. 

Thus, we re-run the training without the juvenile crimes.   
   

## D.4 Random Forest (Violent recidivism)

#### Violent Random Forest Model
```{r}
#-----------------------------------------------VIOLENT RANDOM FOREST-----------------------------------------------
rf_object_vio <- randomForest(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + crime_factor + priors_count + juv_fel_count + juv_misd_count + juv_other_count, data=train.rose,importance=TRUE)

print(rf_object_vio)
varImpPlot(rf_object_vio)

# Training Accuracy
rf_pred_train_vio <- predict(rf_object_vio,
                   type="class" )
l.1 <- confusionMatrix(as.factor(rf_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
l.1

# Test Accuracy
rf_pred_test_vio <- predict(rf_object_vio,
                   newdata = test,
                   type="class")
l <- confusionMatrix(as.factor(rf_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
l 

```

#### Violent Random Forest Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_rf_labels = rf_pred_test_vio
test_with_labels.rf.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_rf_labels)

# Write rf predictions to cox-test 
cox.test.vio.rf <- inner_join(cox.test,test_with_labels.rf.vio,by="id") 
cox.test.vio.rf <- cox.test.vio.rf %>%
  mutate(vio_score_factor_rf = factor(violent_rf_labels)) %>%    
  within(vio_score_factor_rf <- relevel(vio_score_factor_rf, ref = 1))


rf.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_rf
vio_rf_model <- coxph(rf.cox.vio, data=cox.test.vio.rf)
summary(vio_rf_model)
```

> Violent Random Forest: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( l.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(l.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l.1[["byClass"]][["Specificity"]],3)`%|
|Test| `r 100* round(vio_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( l[["overall"]][["Accuracy"]],3)`%|`r 100* round(l[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Random Forest** model performed better in the test set on the accuracy. Similar to the decision tree model, the most important predictors are number of prior crimes, age and gender.

## D.5 Logistic Regression (Violent recidivism)

#### Violent Logistic Regression Model
```{r, cache = TRUE}
#--------------------------------------VIOLENT LOGISTIC REGRESSION MODEL--------------------------------------
violent.log <- glm(is_violent_recid ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count, family="binomial", data=train.rose)


summary(violent.log)
varImp(violent.log)

# Training Accuracy
log_pred_train_vio <- predict(violent.log,type="response")
log_pred_train_vio.pred <- rep(0, length(log_pred_train_vio ))
log_pred_train_vio.pred[log_pred_train_vio  > 0.5] <- 1
m.1 <- confusionMatrix(as.factor(log_pred_train_vio.pred),as.factor(train.rose$is_violent_recid),positive = "1")
m.1

# Test Accuracy
log_pred_test_vio <- predict(violent.log,newdata = test,type="response")
log_pred_test_vio.pred <- rep(0, length(log_pred_test_vio))
log_pred_test_vio.pred[log_pred_test_vio > 0.5] <- 1
m <- confusionMatrix(as.factor(log_pred_test_vio.pred),as.factor(test$is_violent_recid),positive = "1")
m

```

#### Violent Logistic Regression Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_log_labels = log_pred_test_vio.pred
test_with_labels.log.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_log_labels)

# Write log predictions to cox-test 
cox.test.vio.log <- inner_join(cox.test,test_with_labels.log.vio,by="id") 
cox.test.vio.log <- cox.test.vio.log %>%
  mutate(vio_score_factor_log = factor(violent_log_labels)) %>%    
  within(vio_score_factor_log <- relevel(vio_score_factor_log, ref = 1))

log.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_log
vio_log_model <- coxph(log.cox.vio, data=cox.test.vio.log)
summary(vio_log_model)
```

> Violent Logisitcs Regression: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round( m.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(m.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m.1[["byClass"]][["Specificity"]],3)`%|
|Test|`r 100* round(vio_log_model[["concordance"]][["concordance"]],3)`%|`r 100* round( m[["overall"]][["Accuracy"]],3)`%|`r 100* round(m[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m[["byClass"]][["Specificity"]],3)`%|

For violent recidivism, the **Logistics Regression** model performed better in the test set on the accuracy,sensitivity and specificity. 

## D.6 KNN (Violent recidivism)

#### Violent KNN Model

To find the optimal K in KNN, we will use 10-fold cross-validation

```{r, cache = TRUE}
#--------------------------------------VIOLENT KNN MODEL--------------------------------------
# To find the best K 
ctrl <- trainControl(method = "cv", number = 10, 
                     savePredictions = TRUE)

violent.knn <- train(as.factor(is_violent_recid) ~ gender_factor + age_factor + race_factor + priors_count + crime_factor+ juv_fel_count + juv_misd_count + juv_other_count,  data=train.rose, method="knn", trControl = ctrl, tuneLength = 150)

violent.knn
varImp(violent.knn)
plot(violent.knn)

# Training Accuracy
knn_pred_train_vio <- predict(violent.knn)
n.1 <- confusionMatrix(as.factor(knn_pred_train_vio),as.factor(train.rose$is_violent_recid),positive = "1")
n.1

# Test Accuracy
knn_pred_test_vio <- predict(violent.knn, newdata = test)
n <-  confusionMatrix(as.factor(knn_pred_test_vio),as.factor(test$is_violent_recid),positive = "1")
n

```

#### Violent KNN Cox Concordance (Test)
```{r, cache = TRUE}
# Create new dataframe test_with_labels that appends labels to test dataframe
test_with_labels$violent_knn_labels = knn_pred_test_vio
test_with_labels.knn.vio <- test_with_labels %>%
   dplyr::select(id,name,violent_knn_labels)

# Write knn predictions to cox-test 
cox.test.vio.knn <- inner_join(cox.test,test_with_labels.knn.vio,by="id") 
cox.test.vio.knn <- cox.test.vio.knn %>%
  mutate(vio_score_factor_knn = factor(violent_knn_labels)) %>%    
  within(vio_score_factor_knn <- relevel(vio_score_factor_knn, ref = 1))

knn.cox.vio <- Surv(start, end, event.vio, type="counting") ~ vio_score_factor_knn
vio_knn_model <- coxph(knn.cox.vio, data=cox.test.vio.knn)
summary(vio_knn_model)

```


> Violent KNN: Training vs Test 

| Type   | Cox Concordance | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Training||`r 100* round(n.1[["overall"]][["Accuracy"]],3)`%|`r 100* round(n.1[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n.1[["byClass"]][["Specificity"]],3)`%| 
|Test|`r 100* round(vio_knn_model[["concordance"]][["concordance"]],3)`%  |`r 100* round(n[["overall"]][["Accuracy"]],3)`%|`r 100* round(n[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n[["byClass"]][["Specificity"]],3)`%| 

For violent recidivism, the **KNN** model performed worse in the test set on the accuracy,sensitivity, and specificity. The k chosen is `r violent.knn[["bestTune"]][["k"]]`.

# E.1 Task 1 & 2 Overall Comparision

> General Recidivism (Test Set) 

| Model    | Cox Concordance  | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Propublica| `r 100* round(model.pro.cox.gen[["concordance"]][["concordance"]],3)`%| `r 100* round( a[["overall"]][["Accuracy"]],3)`%|`r 100* round(a[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(a[["byClass"]][["Specificity"]],3)`%|
|LDA| `r 100* round(gen_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(c[["overall"]][["Accuracy"]],3)`%|`r 100* round(c[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(c[["byClass"]][["Specificity"]],3)`%|
|QDA| `r 100* round(gen_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( d[["overall"]][["Accuracy"]],3)`%|`r 100* round(d[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(d[["byClass"]][["Specificity"]],3)`%|
|Decision Tree| `r 100* round(gen_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( e[["overall"]][["Accuracy"]],3)`%|`r 100* round(e[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(e[["byClass"]][["Specificity"]],3)`%|
|Random Forest| `r 100* round(gen_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( f[["overall"]][["Accuracy"]],3)`%|`r 100* round(f[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(f[["byClass"]][["Specificity"]],3)`%|
|Logistic Regression| `r 100* round(gen_log_model[["concordance"]][["concordance"]],3)`%   |`r 100* round( g[["overall"]][["Accuracy"]],3)`%|`r 100* round(g[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(g[["byClass"]][["Specificity"]],3)`%|
|KNN| `r 100* round(gen_knn_model[["concordance"]][["concordance"]],3)`%        |`r 100* round(h[["overall"]][["Accuracy"]],3)`%|`r 100* round(h[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(h[["byClass"]][["Specificity"]],3)`%|

> Violent Recidivism (Test Set) 

| Model    | Cox Concordance    | Accuracy | Sensitivity | Specificity| 
|----------|-------|----------|-------------|---------------------------------|
|Propublica| `r 100* round(model.pro.cox.vio[["concordance"]][["concordance"]],3)`%| `r 100* round( b[["overall"]][["Accuracy"]],3)`%|`r 100* round(b[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(b[["byClass"]][["Specificity"]],3)`%|
|LDA| `r 100* round(vio_lda_model[["concordance"]][["concordance"]],3)`%|`r 100* round( i[["overall"]][["Accuracy"]],3)`%|`r 100* round(i[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(i[["byClass"]][["Specificity"]],3)`%|
|QDA| `r 100* round(vio_qda_model[["concordance"]][["concordance"]],3)`%|`r 100* round(j[["overall"]][["Accuracy"]],3)`%|`r 100* round(j[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(j[["byClass"]][["Specificity"]],3)`%|
|Decision Tree| `r 100* round(vio_dt_model[["concordance"]][["concordance"]],3)`%|`r 100* round( k[["overall"]][["Accuracy"]],3)`%|`r 100* round(k[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(k[["byClass"]][["Specificity"]],3)`%|
|Random Forest| `r 100* round(vio_rf_model[["concordance"]][["concordance"]],3)`%|`r 100* round( l[["overall"]][["Accuracy"]],3)`%|`r 100* round(l[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(l[["byClass"]][["Specificity"]],3)`%|
|Logistic Regression|`r 100* round(vio_log_model[["concordance"]][["concordance"]],3)`%|`r 100* round( m[["overall"]][["Accuracy"]],3)`%|`r 100* round(m[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(m[["byClass"]][["Specificity"]],3)`%|
|KNN|`r 100* round(vio_knn_model[["concordance"]][["concordance"]],3)`%     |`r 100* round(n[["overall"]][["Accuracy"]],3)`%|`r 100* round(n[["byClass"]][["Sensitivity"]],3)`%| `r 100* round(n[["byClass"]][["Specificity"]],3)`%|

# F.1 Task 1 & 2 Most Important Predictors
